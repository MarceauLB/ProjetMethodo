p <- dim(X)[2]
lambda_seq <- numeric(K_fold)
# Fixed grid of lambdas on the whole sample
res_samQL <- samQL(X, Y)
grid_lamb <- res_samQL$lambda
# Iterations (using the previous grid of lambdas)
for(k in 1:K_fold){
X_indice_train <- sample(c(1:n),size = round(0.8*n),replace = FALSE)
X_train <- X[X_indice_train,]
Y_train <- Y[X_indice_train]
X_val <- X[-X_indice_train,]
Y_val <- Y[-X_indice_train]
res_samQL <- samQL(X_train, Y_train, lambda = grid_lamb)
prediction <- predict(res_samQL,X_val)
y_pred_val <- prediction$values
best_lambda_index <- which.min(colMeans((y_pred_val - Y_val)**2))
lambda_seq[k] <- grid_lamb[best_lambda_index]
}
lambda_opt <- mean(lambda_seq)
return(lambda_opt)
}
selected_redundant_index_OVA <- function(X,Y){
classes <- unique(Y)
for(classe in classes){
print(classe)
}
}
selected_redundant_index_OVA(X,Y)
classes
selected_redundant_index_OVA <- function(X,Y){
classes <- unique(Y)
for(classe in classes){
print(classe)
}
}
selected_redundant_index_OVA <- function(X,Y){
classes <- unique(Y)
classes <- as.numeric(classes)
for(classe in classes){
print(classe)
}
}
selected_redundant_index_OVA(X,Y)
redundant_index <- matrix(nrow = length(combinations),ncol = 45)
classe
selected_redundant_index_OVA <- function(X,Y){
classes <- unique(Y)
classes <- as.numeric(classes)
for(classe in classes){
# sous jeu de données pour One vs All
Y_coded <- ifelse(Y==classe,1,0)
print(Y_coded)
}
}
selected_redundant_index_OVA(X,Y)
res_sam <- samLL(X,Y_coded)
res_sam
res_sam$lambda
lambda_grid <- res_sam$lambda
get_lambda_optimal <- function(X,Y,K_fold=5,grid_lamb){
n <- dim(X)[1]
p <- dim(X)[2]
# Fixed grid of lambdas on the whole sample
res_samQL <- samQL(X, Y)
grid_lamb <- res_samQL$lambda
# Iterations (using the previous grid of lambdas)
for(k in 1:K_fold){
X_indice_train <- sample(c(1:n),size = round(0.8*n),replace = FALSE)
X_train <- X[X_indice_train,]
Y_train <- Y[X_indice_train]
X_val <- X[-X_indice_train,]
Y_val <- Y[-X_indice_train]
res_samLL <- samQL(X_train, Y_train, lambda = grid_lamb)
prediction <- predict(res_samQL,X_val)
y_pred_val <- prediction$values
best_lambda_index <- which.min(colMeans((y_pred_val - Y_val)**2))
lambda_seq[k] <- grid_lamb[best_lambda_index]
}
lambda_opt <- mean(lambda_seq)
return(lambda_opt)
}
res <- samLL(X,Y_coded,lambda = 0.048952698 )
res
index <- order(res$func_norm,decreasing = TRUE)
index
res$func_norm[index][1:50]
index <- order(res$func_norm,decreasing = TRUE)[1:50]
index
res$func_norm[index][1:50]
index_selected
selected_indices
selected_indices <- matrix(rpois(20*5,10),nrow = 5,ncol = 20)
selected_indices
selected_indices <- matrix(rpois(20*5,10),nrow = 5,ncol = 20)
selected_indices
selected_indices
as.vector(selected_indices)
total_selected <- as.vector(selected_indices)
table(total_selected)
sort(table(total_selected))
sort(table(total_selected),decreasing = TRUE)
sort(table(total_selected),decreasing = TRUE)[1:3]
names(sort(table(total_selected),decreasing = TRUE)[1:3])
as.numeric(names(sort(table(total_selected),decreasing = TRUE)[1:3]))
top_indices <- as.numeric(names(sort(table(total_selected),decreasing = TRUE)[1:3]))
top_indices
top_indices[2]
library(caret)
Y
ifelse(Y==1,1,0)
# Stratified Sampling
train_idx <- createDataPartition(ifelse(Y==1,1,0), p = 0.8, list = FALSE)
train_idx
# Stratified Sampling
?createDataPartition
train_idx <- createDataPartition(ifelse(Y==1,1,0), p = 0.8, list = FALSE,groups = 2)
train_idx
train_idx <- createDataPartition(ifelse(Y==1,1,0), p = 0.8, list = FALSE,times = 2)
train_idx
train_idx <- createDataPartition(ifelse(Y==1,1,0), p = 0.8, list = FALSE)
train_idx
ifelse(Y==1,1,0)
train_idx <- createDataPartition(ifelse(Y==1,1,0), p = 0.8, list = FALSE)
train_idx
train_idx
train_idx
train_idx <- createDataPartition(ifelse(Y==1,1,0), p = 0.8, list = FALSE,times=5)
train_idx
class(train_idx)
train_idx[,1]
Y_train
train_idx[,1]
X_indice_train <- train_idx[,1]
X_train <- X[X_indice_train,]
Y_train <- Y[X_indice_train]
Y_train <- Y[X_indice_train]
Y_train
Y_train <- Y_coded[X_indice_train]
Y_train
X_val <- X[-X_indice_train,]
Y_val <- Y_coded[-X_indice_train]
Y_val
sam_boost <- samLL(X_train,Y_train,lambda = grid_lamb)
sam_boost
sam_boost <- samLL(X_train,Y_train,lambda = grid_lamb)
grid_lamb
# sous jeu de données pour One vs All
Y_coded <- ifelse(Y==classe,1,0)
classes <- unique(Y)
classes <- as.numeric(classes)
# sous jeu de données pour One vs All
Y_coded <- ifelse(Y==classe,1,0)
classe
# sous jeu de données pour One vs All
Y_coded <- ifelse(Y==1,1,0)
Y_coded
res_sam <- samLL(X,Y_coded)
lambda_grid <- res_sam$lambda
lambda_grid
lambda_grid
sam_boost <- samLL(X_train,Y_train,lambda = lambda_grid)
sam_boost
prediction <- predict(sam_boost,X_val)
prediction
prediction <- predict(sam_boost,X_val)
prediction <- predict(sam_boost,X_val)
prediction <- predict(sam_boost,X_val)
X_val
prediction <- predict(sam_boost,X_val)
source("~/00_Ensai/projet-methodo/ProjetMethodo/real-word-datasets/ar10p-mca-red-spam.R", echo=TRUE)
prediction
prediction
prediction
prediction
prediction
prediction
prediction <- predict(sam_boost,X_val)
sam_boost <- samLL(X_train,Y_train,lambda = lambda_grid)
X_indice_train <- train_idx[,1]
lambda_seq <- numeric(K_fold)
lambda_seq
get_lambda_optimal <- function(X,Y_coded,K_fold=5,lambda_grid){
n <- dim(X)[1]
p <- dim(X)[2]
lambda_seq <- numeric(K_fold)
train_idx <- createDataPartition(ifelse(Y==1,1,0), p = 0.8, list = FALSE,times=K_fold)
# Iterations (using the previous grid of lambdas)
for(k in 1:K_fold){
X_indice_train <- train_idx[,1]
X_train <- X[X_indice_train,]
Y_train <- Y_coded[X_indice_train]
X_val <- X[-X_indice_train,]
Y_val <- Y_coded[-X_indice_train]
sam_boost <- samLL(X_train,Y_train,lambda = lambda_grid)
prediction <- predict(sam_boost,X_val)
y_pred_val <- prediction$values
best_lambda_index <- which.min(colMeans((y_pred_val - Y_val)**2))
lambda_seq[k] <- lambda_grid[best_lambda_index]
}
lambda_opt <- mean(lambda_seq)
return(lambda_opt)
}
index_feature_OVA <- function(X,Y){
redundant_index <- matrix(nrow =10,ncol = 50)
classes <- unique(Y)
classes <- as.numeric(classes)
for(classe in classes){
# sous jeu de données pour One vs All
Y_coded <- ifelse(Y==classe,1,0)
res_sam <- samLL(X,Y_coded)
lambda_grid <- res_sam$lambda
# trouver le lambda optimal
get_best_lambda <- get_lambda_optimal(X,Y_coded,K_fold = 5,lambda_grid)
# entrainer le modèle finale avec le meilleur lambda
final_SAM <- samLL(X,Y_coded,lambda = get_best_lambda)
index_selected <- order(final_SAM$func_norm,decreasing = TRUE)[1:50]
# stocker les index des lambdas
redundant_index[classe,] <- index_selected
}
# 50 indices les plus redondants parmi toutes les lignes
total_selected <- as.vector(redundant_index)
top_indices <- as.numeric(names(sort(table(total_selected),decreasing = TRUE)[1:50]))
return(top_indices)
}
rm(list=ls())
library(SAM)
library(combinat)
library(caret)
set.seed(12345)
?samHL
?samLL
library(caret)
X <- read.csv("00_Ensai/projet-methodo/ProjetMethodo/real-word-datasets/dataset_csv/ar10p.csv")
n <- dim(X)[1]
p <- dim(X)[2]-1
Y <- as.factor(X[,-(1:p)])
X <- X[,-(p+1)]
X <- as.data.frame(scale(X,center = TRUE,scale = TRUE))
X <- as.matrix(X)
get_lambda_optimal <- function(X,Y_coded,K_fold=5,lambda_grid){
n <- dim(X)[1]
p <- dim(X)[2]
lambda_seq <- numeric(K_fold)
train_idx <- createDataPartition(ifelse(Y==1,1,0), p = 0.8, list = FALSE,times=K_fold)
# Iterations (using the previous grid of lambdas)
for(k in 1:K_fold){
X_indice_train <- train_idx[,1]
X_train <- X[X_indice_train,]
Y_train <- Y_coded[X_indice_train]
X_val <- X[-X_indice_train,]
Y_val <- Y_coded[-X_indice_train]
sam_boost <- samLL(X_train,Y_train,lambda = lambda_grid)
prediction <- predict(sam_boost,X_val)
y_pred_val <- prediction$values
best_lambda_index <- which.min(colMeans((y_pred_val - Y_val)**2))
lambda_seq[k] <- lambda_grid[best_lambda_index]
}
lambda_opt <- mean(lambda_seq)
return(lambda_opt)
}
index_feature_OVA <- function(X,Y){
redundant_index <- matrix(nrow =10,ncol = 50)
classes <- unique(Y)
classes <- as.numeric(classes)
for(classe in classes){
# sous jeu de données pour One vs All
Y_coded <- as.factor(ifelse(Y==classe,1,0))
res_sam <- samLL(X,Y_coded)
lambda_grid <- res_sam$lambda
# trouver le lambda optimal
get_best_lambda <- get_lambda_optimal(X,Y_coded,K_fold = 5,lambda_grid)
# entrainer le modèle finale avec le meilleur lambda
final_SAM <- samLL(X,Y_coded,lambda = get_best_lambda)
index_selected <- order(final_SAM$func_norm,decreasing = TRUE)[1:50]
# stocker les index des lambdas
redundant_index[classe,] <- index_selected
}
# 50 indices les plus redondants parmi toutes les lignes
total_selected <- as.vector(redundant_index)
top_indices <- as.numeric(names(sort(table(total_selected),decreasing = TRUE)[1:50]))
return(top_indices)
}
selected_redundant_index_OVA(X,Y)
rm(list=ls())
library(SAM)
library(combinat)
library(caret)
set.seed(2303)
X <- read.csv("ar10p.csv")
data <- matrix(rpois(10*3,100),nrow = 3,ncol = 10)
data
data[2,9] <- NA
data
data[3,10] <- NA
data
as.vector(data)
table(as.vector(data))
data <- matrix(rpois(15*4,100),nrow = 3,ncol = 10)
data <- matrix(rpois(15*4,100),nrow = 3,ncol = 10)
data <- matrix(rpois(15*4,100),nrow = 4,ncol = 15)
data
table(as.vector(data))
data <- matrix(rpois(15*4,100),nrow = 4,ncol = 15)
data[3,10] <- NA
data <- matrix(rpois(15*4,100),nrow = 4,ncol = 15)
data
table(as.vector(data))
data[3,7] <- NA
data
table(as.vector(data))
setwd("~/00_Ensai/projet-methodo/ProjetMethodo/real-word-datasets/res_mca_red_ar10p/")
rm(list=ls())
mrmr <- read.csv("ar10p_mrmr_mca.csv", sep="", header=FALSE)
lasso <- read.csv("ar10p_lasso_mca.csv", sep="", header=FALSE)
enet <- read.csv("ar10p_enet_mca.csv", sep="", header=FALSE)
ckta <- read.csv("ar10p_ckta_mca.csv", sep="", header=FALSE)
ckta <- ckta[1:13,]
qpfs <- read.csv("ar10p_qpfs_mca.csv", sep="", header=FALSE)
hsic <- read.csv("res_hsic/ar10p_hsic_mca_sigma1.csv")
spam <- read.csv("ar10p_spam_mca.csv")
spam
red_spam <- read.csv("ar10p_spam_red50.csv")
red_spam
cmean_mrmr <- colMeans(mrmr)
cmean_lasso <- colMeans(lasso)
cmean_enet <- colMeans(enet)
cmean_ckta <- colMeans(ckta)
cmean_qpfs <- colMeans(qpfs)
cmean_hsic <- rowMeans(hsic)
spam
cmean_spam <- rowMeans(spam)
cmean_spam
# - variance associé à la colonne 5
sqrt(var(mrmr$V5))
sqrt(var(lasso$V5))
sqrt(var(enet$V5))
sqrt(var(ckta$V5))
sqrt(var(qpfs$V5))
hsic <- as.matrix(hsic)
sqrt(var(hsic[5,]))
sqrt(var(spam[5,]))
spam <- as.matrix(spam)
spam
red_spam <- read.csv("ar10p_spam_red50.csv")
cmean_mrmr <- colMeans(mrmr)
cmean_lasso <- colMeans(lasso)
cmean_enet <- colMeans(enet)
cmean_ckta <- colMeans(ckta)
cmean_qpfs <- colMeans(qpfs)
cmean_hsic <- rowMeans(hsic)
cmean_spam <- rowMeans(spam)
cmean_spam
spam
cmean_spam <- rowMeans(spam)
cmean_spam
red_spam <- read.csv("ar10p_spam_red50.csv")
spam <- as.matrix(spam)
red_mrmr <- read.csv("ar10p_mrmr_red50.csv", header=FALSE,sep="")
red_lasso <- read.csv("ar10p_enet_red50.csv", header=FALSE, sep="")
red_enet <- read.csv("ar10p_lasso_red50.csv", header=FALSE, sep="")
red_ckta <- read.csv("ar10p_ckta_red50.csv")
red_qpfs <- read.csv("ar10p_qpfs_red50.csv", header=FALSE, sep="")
red_hsic <- read.csv("res_hsic/ar10p_hsic_red50.csv", header=FALSE, sep=",")
red_spam <- read.csv("ar10p_spam_red50.csv")
cmean_mrmr <- colMeans(mrmr)
cmean_lasso <- colMeans(lasso)
cmean_enet <- colMeans(enet)
cmean_ckta <- colMeans(ckta)
cmean_qpfs <- colMeans(qpfs)
cmean_hsic <- rowMeans(hsic)
cmean_spam <- rowMeans(spam)
cmean_spam
spam
spam[5,]
var(spam[5,])
sqrt(var(spam[5,]))
red_spam <- read.csv("ar10p_spam_red50.csv")
red_spam
# red score mean and S.D
red_mrmr_vec <- as.vector(as.matrix(red_mrmr))
red_lasso_vec <- as.vector(as.matrix(red_lasso))
red_enet_vec <- as.vector(as.matrix(red_enet))
red_ckta_vec <- as.vector(as.matrix(red_ckta))
red_qpfs_vec <- as.vector(as.matrix(red_qpfs))
red_hsic_vec <- as.vector(as.matrix(red_hsic))
red_spam_vec <- as.vector(as.matrix(red_spam))
red_spam_vec
red_spam_vec <- as.vector(as.matrix(red_spam))
red_spam_vec
mean(red_mrmr_vec)
sqrt(var(red_mrmr_vec))
mean(red_lasso_vec)
sqrt(var(red_lasso_vec))
mean(red_enet_vec)
sqrt(var(red_enet_vec))
mean(red_ckta_vec)
sqrt(var(red_ckta_vec))
mean(red_qpfs_vec)
sqrt(var(red_qpfs_vec))
mean(red_hsic_vec)
sqrt(var(red_hsic_vec))
mean(red_spam_vec)
sqrt(var(red_spam_vec))
features_index <- seq(10,50,10)
plot(features_index,cmean_hsic,type="l",col="black",ylim=c(0.3,1),
xlab ="Number of extracted features",
ylab="Mean classification accuracy",
lwd=2)
lines(features_index,cmean_enet,type="l",col="orange2",lwd=2)
lines(features_index,cmean_lasso,type="l",col="purple2",lwd=2)
lines(features_index,cmean_ckta,type="l",col="yellow3",lwd=2)
lines(features_index,cmean_qpfs,type="l",col="red2",lwd=2)
lines(features_index,cmean_mrmr,type="l",col="cyan2",lwd=2)
lines(features_index,cmean_hsic,type="l",col="black",lwd=2)
legend("bottomright",
legend = c("HSIC",  "mRMR", "Lasso",     "cKTA", "QPFS", "ENet"),
col = c("black","cyan2", "purple2", "yellow3", "red2", "orange2"),
lwd = 2, cex = 0.8)
lines(features_index,cmean_spam,type="l",col="green2",lwd=2)
lines(features_index,cmean_hsic,type="l",col="black",lwd=2)
legend("bottomright",
legend = c("HSIC",  "mRMR", "Lasso",     "cKTA", "QPFS", "ENet"),
col = c("black","cyan2", "purple2", "yellow3", "red2", "orange2"),
lwd = 2, cex = 0.8)
legend("bottomright",
legend = c("HSIC","SPAM", "mRMR", "Lasso",     "cKTA", "QPFS", "ENet"),
col = c("black","green2","cyan2", "purple2", "yellow3", "red2", "orange2"),
lwd = 2, cex = 0.8)
features_index <- seq(10,50,10)
plot(features_index,cmean_hsic,type="l",col="black",ylim=c(0.3,1),
xlab ="Number of extracted features",
ylab="Mean classification accuracy",
lwd=2)
lines(features_index,cmean_enet,type="l",col="orange2",lwd=2)
lines(features_index,cmean_lasso,type="l",col="purple2",lwd=2)
lines(features_index,cmean_ckta,type="l",col="yellow3",lwd=2)
lines(features_index,cmean_qpfs,type="l",col="red2",lwd=2)
lines(features_index,cmean_mrmr,type="l",col="cyan2",lwd=2)
lines(features_index,cmean_spam,type="l",col="green2",lwd=2)
lines(features_index,cmean_hsic,type="l",col="black",lwd=2)
legend("bottomright",
legend = c("HSIC","SPAM", "mRMR", "Lasso",     "cKTA", "QPFS", "ENet"),
col = c("black","green2","cyan2", "purple2", "yellow3", "red2", "orange2"),
lwd = 2, cex = 0.8)
features_index <- seq(10,50,10)
plot(features_index,cmean_hsic,type="l",col="black",ylim=c(0.3,1),
xlab ="Number of extracted features",
ylab="Mean classification accuracy",
lwd=2)
lines(features_index,cmean_enet,type="l",col="orange2",lwd=2)
lines(features_index,cmean_lasso,type="l",col="purple2",lwd=2)
lines(features_index,cmean_ckta,type="l",col="yellow3",lwd=2)
lines(features_index,cmean_qpfs,type="l",col="red2",lwd=2)
lines(features_index,cmean_mrmr,type="l",col="cyan2",lwd=2)
lines(features_index,cmean_spam,type="l",col="green2",lwd=2)
lines(features_index,cmean_hsic,type="l",col="black",lwd=2)
legend("bottomright",
legend = c("HSIC","SPAM", "mRMR", "Lasso",     "cKTA", "QPFS", "ENet"),
col = c("black","green2","cyan2", "purple2", "yellow3", "red2", "orange2"),
lwd = 2, cex = 0.8)
features_index <- seq(10,50,10)
plot(features_index,cmean_hsic,type="l",col="black",ylim=c(0.3,1),
xlab ="Number of extracted features",
ylab="Mean classification accuracy",
lwd=2)
lines(features_index,cmean_enet,type="l",col="orange2",lwd=2)
lines(features_index,cmean_lasso,type="l",col="purple2",lwd=2)
lines(features_index,cmean_ckta,type="l",col="yellow3",lwd=2)
lines(features_index,cmean_qpfs,type="l",col="red2",lwd=2)
lines(features_index,cmean_mrmr,type="l",col="cyan2",lwd=2)
lines(features_index,cmean_spam,type="l",col="green2",lwd=2)
lines(features_index,cmean_hsic,type="l",col="black",lwd=2)
legend("bottomright",
legend = c("HSIC","SPAM", "mRMR", "Lasso",     "cKTA", "QPFS", "ENet"),
col = c("black","green2","cyan2", "purple2", "yellow3", "red2", "orange2"),
lwd = 2, cex = 0.8)
features_index <- seq(10,50,10)
plot(features_index,cmean_hsic,type="l",col="black",ylim=c(0.3,1),
xlab ="Number of extracted features",
ylab="Mean classification accuracy",
lwd=2)
lines(features_index,cmean_enet,type="l",col="orange2",lwd=2)
lines(features_index,cmean_lasso,type="l",col="purple2",lwd=2)
lines(features_index,cmean_ckta,type="l",col="yellow3",lwd=2)
lines(features_index,cmean_qpfs,type="l",col="red2",lwd=2)
lines(features_index,cmean_mrmr,type="l",col="cyan2",lwd=2)
lines(features_index,cmean_spam,type="l",col="green2",lwd=2)
lines(features_index,cmean_hsic,type="l",col="black",lwd=2)
legend("bottomright",
legend = c("HSIC","SPAM", "mRMR", "Lasso",     "cKTA", "QPFS", "ENet"),
col = c("black","green2","cyan2", "purple2", "yellow3", "red2", "orange2"),
lwd = 2, cex = 0.8)
legend("bottomright",
legend = c("HSIC","SPAM", "mRMR", "Lasso",     "cKTA", "QPFS", "ENet"),
col = c("black","green2","cyan2", "purple2", "yellow3", "red2", "orange2"),
lwd = 2, cex = 1)
features_index <- seq(10,50,10)
plot(features_index,cmean_hsic,type="l",col="black",ylim=c(0.3,1),
xlab ="Number of extracted features",
ylab="Mean classification accuracy",
lwd=2)
lines(features_index,cmean_enet,type="l",col="orange2",lwd=2)
lines(features_index,cmean_lasso,type="l",col="purple2",lwd=2)
lines(features_index,cmean_ckta,type="l",col="yellow3",lwd=2)
lines(features_index,cmean_qpfs,type="l",col="red2",lwd=2)
lines(features_index,cmean_mrmr,type="l",col="cyan2",lwd=2)
lines(features_index,cmean_spam,type="l",col="green2",lwd=2)
lines(features_index,cmean_hsic,type="l",col="black",lwd=2)
legend("bottomright",
legend = c("HSIC","SPAM", "mRMR", "Lasso",     "cKTA", "QPFS", "ENet"),
col = c("black","green2","cyan2", "purple2", "yellow3", "red2", "orange2"),
lwd = 2, cex = 1)
